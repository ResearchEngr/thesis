\newcommand{\mypathdfo}{../thesis/dfo}
\newcommand{\mypathdfodata}{../thesis/dfo/data}
\newcommand{\scd}{\cD_\oplus}
\newcommand{\btu}{\bigtriangleup}
\chapter{Solving Design Problems}

The cascading model built in chapter \ref{msip-model} is too hard to optimize at the scale we need to model the uncertainty in the cascading process.  In order to work around this difficulty, we will decompose the multi-stage structure of this problem and simulate the entire cascading process.  The primary benefit is removing the temporal linked binary variables for line outages from the master problem and allowing for sequential evaluation of the decision dependent uncertainty.  This allows us to parallize the OPA evaluations and use a wider range of potential risk metrics and design problems for optimization as well as reasonable solve times.

In order to optimize design problems using the OPA simulation for risk metrics, we need to first understand the characterestics of the cascading process and the effects that generation, reserves, and line limits have on the resulting load shed distributions.  Then, we can taylor existing derivative free frameworks to optimize this simulation and utilize explotatory steps to improve local performance and global strategies to find near optimal decisions.

The OPA simulation provides fundamental difficulties to the optimization process.  The function evaluations can shown to be no convex nor continuously differentiable every.  It is inherently noisy and the load shed distribution is wide, often charactereized by it's power law distribution.  Some of these effects can be smoothed away by employing a wide range of potential initiating events, however it may be important to optimize against a small subset of events that have a higher likihood of occurance and are known to be risky.  In this case, the non convexity and discontinuoity are most apparent.

To begin, an overview of derivative free optimization will be giving.  Three classes will be looked at for their convergence properties and practicle applications.  Then, initial experimentation on the OPA model will be given as well as the implementation and variance reduction strategies.  Then a modified algorithm will be developed and finally compared against vanilla algorithms of DFO.  A primary undercurrent is the use of accessory information in the modified DFO algorithm.

\section{Literature Review}

\subsection{Derivative Free Optimization}


\begin{equation}
\min_{x \in \R{n}} f(x)
\end{equation}
history
different types
strength/weakness
relevant literature for more information

\begin{figure}
\centering
\input{\mypathdfo/fig-dfomethod}
\caption{DFO methods for continuous variable optimization}
\end{figure}

\subsection{Direct Search}
convergence in gradient line search methods

-decent direction
\begin{equation}
-\grad f(x_k)^T d_k > 0
\end{equation}
Good direction cosine measure, angle condition
\begin{equation}
\frac{-\grad f(x_k)^T d_k}{\norm{\grad f(x_k) } \norm{d_k}} \geq c > 0
\end{equation}
automatically holds for steepest descent, where $d=-\grad f(x)$

-step length Armijo-Goldstein-Wolfe conditions
\begin{align}
f(x_k + \alpha_k d_k) \le f(x_k) + c_1 \alpha_k \grad f(x_k)^T d_k  \label{step_length_1}\\
\grad f (x_k + \alpha_k d_k)^T d_k \ge c_2 \grad f(x_k)^T d_k \label{step_length_2}
\end{align}
with $0 < c_1 < c_2 < 1$


\subsection*{Now Without Derivatives}
how do we get around no derivative
positive spanning set (picture)
\begin{figure}
\centering
\input{\mypathdfo/fig-spanset}
\caption{Positive spanning sets for $\R{2}$}
\end{figure}
-characterize good spaning set, cosing measure
-generating set for nearby active constraints (region a cone)
\begin{figure}
\centering
\input{\mypathdfo/fig-genset}
\caption{Generating set for polar cone to conform to local topology}
\end{figure}
with bound constraints, compass direction good at exploring boundary of feasible region

show general GSS algorithm

\begin{algorithm}
\caption{Compass search, a generating set search,Kolda et. al. \cite{kolda_2003}}\label{dfo_genset}
\begin{algorithmic}
\Procedure{CS}{$f:\R{n} \rightarrow \R{}$}
\State $x_0 \in \R{n}$ Initial guess
\State $\bigtriangleup_{tol} >0$ Termination criteria
\State $\bigtriangleup_0 > \bigtriangleup_{tol} >0$ Initial neighborhood
\BState For each $k=1,2,...$

\State Let $\scd =\left\{ \pm e_i| i=1,...,n\right\}$ be the set of coordinate directions
\If{ $\exists d_k \in \scd$ such that $f(x_k + \btu_k d_k ) < f(x_k)$}
\State $x_{k+1} \gets x_k + \btu_k d_x$
\State $\btu_{k+1} = \btu_k$
\Else
\State $x_{k+1} \gets x_k$
\State $\btu_{k+1} = \frac{1}{2} \btu_k$
\If{$\btu_{k+1} < \btu_{tol}$} terminate \EndIf
\EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}

properties that allow for converge
sketch of proof
exploratory steps

\subsection*{Mesh Adaptive}
Conn pg 140

\subsection{Model Based Search}
trust regions
Stochastic Trust Region \endnote{\input{\mypathdfo/end-stochtrust}}

\subsection*{Gradient Models}
Implicit filtering
Simplex gradients
Stochastic Approximation

Stochastic Gradient \endnote{\input{\mypathdfo/end-stochgrad}}

Stochastic Approximation \endnote{\input{\mypathdfo/end-stochapprox}}





\section{Optimizing the OPA Simulation}


\begin{itemize}
\item Develop an efficient evaluation of cascading power failures for use in optimization procedure
\begin{itemize}
\item Computational implementation
\item Variance reduction
\item Large scale problems
\end{itemize}
\item Investigate the behavior of this model
\begin{itemize}
\item Lack of convexity for topology changes
\item Non-monotonic behavior for line capacity additions
\item Poor 1st stage approximators of total load shed
\end{itemize}
\item Create algorithm to find good solutions to design problems
\begin{itemize}
\item Finding good search directions
\item Line search procedure for search directions
\item Incorporation of problem specific information
\end{itemize}
\end{itemize}





\begin{comment}
\subsection*{Simulation Implementation}

This is how the simulation is done

\subsection*{Implementation}
Parallel computation - condor
C++ simulation code
Python code for analysis

\subsection{Variance Reduction}
Important for reducing computation time
Dobson splitting method, type of importance sampling
sampling techniques

\subsubsection{Common Random Numbers}
reference textbook on simulation Simulation Modeling and Analysis (Averill M. Law) \cite{law_2007}


\subsection*{Model Behavior}
To investigate the behavior of the model, we will begin by looking at the results from a single contingency.  Once this is better understood, the effects of multiple possible contingencies will be looked at.
\subsection*{Topology Information}
anything can happen
\subsubsection{Clustering}
Lines tend to behave in groups which relate to underlying topology.
Take advantage by looking at correlation between load shed and line outages

\begin{figure}
\centering
\input{\mypathdfo/fig-linecluster}
\caption{A cluster of a lines responsible for reducing system performance}
\label{fig:cluster}
\end{figure}

This information can be used to aid in the simulation process.  By supplementing this information with topology information, interesting conclusions can be drawn.

\subsection{Power-Law Failure Distribution}
risk measures
The load shed from power outages follows a power law distribution.


\subsection*{Adding Capacity}
To begin, we will look at what happens when you double capacity on each line.  (\ref{fig:grad})  Out of the 186 lines, 51 lines resulted in an improved system with respect to the cascading process, 47 were within the margin of error of the nominal system, and 88 reduced the system performance.
\begin{figure}
\input{\mypathdfo/fig-double}
\end{figure}



Lets look along a line in each group.
\begin{figure}
\input{\mypathdfo/fig-capadd}
	\caption{Plotting expected load shed for capacity additions along coordinate directions.  }
\end{figure}



\subsection*{Risk Measures}
\begin{figure}
\input{\mypathdfo/fig-riskmeasure}
\caption{Value at Risk, Conditional Value at Risk, and Maximum risk measures for load shed distribution}
\end{figure}

The value at risk and conditional value at risk tend to track in a similar manner to the expect value.  The maximum can display much more erratic behavior and is unreasonable in a realistic setting with cost constraints.

results
reserve allocations?


\subsection*{1st Stage Approximators}
Any good way to approximate the total effect of the cascading process?

Correlation between number of lines outaged in 1st stage and the total load shed.
\begin{figure}
\centering
\input{\mypathdfo/fig-firststage}
\caption{Relationship between load shed and the number of lines outaged in the first stage.}
\label{fig:first}
\end{figure}

Correlation between

\begin{figure}
\centering
\input{\mypathdfo/fig-loadserve}
 \caption{Load Served at Different Stages}
\end{figure}

Good news, can estimate relative effects of cascade on different systems
1st stage line flows
\begin{figure}
\begin{subfigure}[b]{\textwidth}
\centering
\input{\mypathdfo/fig-lineflowscene}
\end{subfigure}
\begin{subfigure}[b]{\textwidth}
\begin{center}
\input{\mypathdfo/fig-failprobscene}
\end{center}
\end{subfigure}
 \caption{Line flows and failure probabilities for each scenario and  their averages}
\label{fig:flows}
\end{figure}

This information can be used to tell if two operating points have different cascading properties.  By looking at the distance between two operating points, breakpoints can be found where the distance between operating points risk characteristics are extremely different from another, close, operating point.

\section{Algorithm Design}
\subsection{Search Direction and Break Points}
most outaged lines
refine with 	-topology information
		-electrical properties
clustering
		-line correlations
\subsection*{Line Search}
breakpoint analsis


\begin{figure}
\centering
\input{\mypathdfo/fig-breakpoint}
  \caption{Approximation of function by finding breakpoints}
\label{fig:break}
\end{figure}



choosing N
\subsection{Local Convergence}
derivative free techniques
find neighborhood with continuity
drive towards local optimal
search basis
exploratory steps

\subsection{Modified Coordinate Search}
All together now

\section{Computational Results}


\subsection{Vanilla Optimization}


\subsection{MCS Implementation}
Negative - No local convergence properties
Old problem parameters, which is why the scales are so different

\begin{figure}
\begin{center}
\input{\mypathdfo/fig-optroute}
 \caption{Brute force search procedure along coordinate directions in the null space}
 \label{fig:opt1}
\end{center}
\end{figure}



\section{Conclusion}


Covariance matrix
try instead of solving economic dispatch, solve min variance problem ??? analogous to CAPM, need to make modifcation though!!!! its all related sooo close


look for a set of net power injects wich minimizes the variance in revenue/cost at all of the nodes, perhaps weighted by centrallity, otherwise weight evenly?
what are characterstics of this set versus the one formed from economic dispatch over day ahead and real time markets
what is difference in cost
what is difference in reliability



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% old
stochastic vs deterministic
stochastic 
-simulation based
Berry Nelson

stochastic
-opt community
Guanghai Lan, 
Sample Approximation Average
Random Gradient Descent
Robust Stochastic Approximation Nemirouski Jouditsty Lan Shapiro
Adaptive Robust Optimization for security constrained unit commitment Bertsimas Lituinov Sun Zhao Zheng
Introduction to stochastic search and optimization Spall

Stochastic versions (get stochastic book)
Sample Approximation Average
robust stochastic approximation (Nemirovsky Juditsty, Lan, Shapiro)
Spoll  -Introduction to stochastic search and Optimization
Adaptive Robust optimization for Security Constrained Unit Commitment  (Bertsimas, Lituinov, Sun, Zhao, Zheng)


\subsubsection{Pattern Search}
deterministic
-pattern search
Generating Set Search (Kolda, Lewis, Torczon Review 2003 convergence results)
derivative free optimization book (Conn Scheinberg Vicente 2003)
Simplex


\subsubsection{Model Based}
deterministic
-model based (Conn Scheinberg Vicente 2003) for all of these
interpolation
regression
trust regions


derivative free optimization
derivate used - could give examples of it not working <<< more mork
local model based


Nonlinear Optimization \endnote{\input{\mypathdfo/end-nonline}}



%%%%%%%%%%%%%%





--pattern search, using secondary simulation data, model based?, evolutionary, trust regions

Direct Search Methods: then and now ( Lewis, Torczon, Trosset ) \cite{lewis_2000}

\begin{equation}
\min_{ f: \R{n} \rightarrow \R{}} f(x)
\end{equation}
f continuously differentiable but unavailable

pattern search methods
simplex method (geometry based search strategies)
adaptive sets of search directions

some history on convergence results

Polak '71 specific algorithm converge to accumlation point $x'$ of $\left\{x_k\right\}$ such tthat 
\begin{equation}
\bigtriangledown f(x') = 0
\end{equation}

Berman provides results for fairly general algorithm based on lattice method

initialize 
$x_0, \Delta_0$
\begin{equation}
L(x_0, \Delta_0) = \left\{ x | x = x_0 + \Delta_0 \lambda, \lambda \in \Lambda \right\}
\end{equation}
where $\Lambda $ is a lattice of integral points on $\R{n}$
if $L_k  = L(x_k, \Delta_k)$, $\Delta_k = \frac{\delta_0}{\tau^k}$, $\tau > 1$ and $L_k \subset L_{k+1}$
as $k \uparrow$ , $\Delta \downarrow$, as $k$ increases, resolution increases

Cea gives convergence results on Hookes and Jeeves algorithm, for strictly convex function, you get a unique minimizer

V. Torczon generalizes Bermans results
patterns of form 
\begin{equation}
x_k' = x_k + \Delta_k B \gamma_k'
%%%x_k^' = x_k + \Delta_k \Beta \gamma_k^'
\end{equation}
where $B$ is a nonsingular matrix and $\gamma_k'$ is an integral vector
Assume that $L(x_0) = \left\{ x | f(x)  \le f(x_0) \right\} $ is compact, f cont. diff. on neighborhood of $L(x_0)$.  Then sequance of iterats $\left\{ x_k \right\}$ produced by algorithm has
\begin{equation}
\lim_{k \rightarrow \infty} \inf || \bigtriangledown f (x_k) || = 0
\end{equation}


Nelder Mead, expansion, contraction, reflection
Adaptive set of search direction ( Rosenbrock Powell)

%%%%%%%%%%%%%%%%%
Optimization by Direct Search: New Perspectives on Some Classical and Modern Methods (Kolda, Lewis, Torczon) \cite{kolda_2003}


search methods due to stochastic error 6,173,260
$f\in C^1$ - derivative is continuously differntiable, gradient method
$f \in C^2$ - twice differentiable, Newton based methods 69, 192, 197

direct search methods, noisy numerical error
issues
-slow asymptotic convergence
-problem size

derivative free 71,74, 177
-models via least squares, regression 121
-interpolation models 72, 74, 220
-no explicit model 115, 173, 174, 175

Direct Search - sequential examination of trial solutions involving comparison of trial solutions with the "best" obtained up to that time together with a strategy for determing next trial solutions (as a function of earlier results)


assume order relation
$ x \prec y$ if $f(x) < f(y)$
only finite amt of new iterates and can be enumerated

parallel implementation 93, 115, 140, 256
-load balancing 140
161,162

Hybrid approach
direct search with
-trust region 141
-evolutionary alg 131,132,133
-modeling acceleration technique 30, 31, 91, 100, 261

Smooth Unconstrained Minimization

Line search
if $f$ differntiable at $x$, $d$ is a descent direction for $f$ at $x$ 
\begin{equation}
- \grad f(x)^T d > 0
\end{equation}
and if $d$ is descent, $\alpha >0$ and sufficiently small, $x_t$ reduces value of $f$
\begin{equation}
f(x + \alpha d) = f(x) + \alpha \grad f(x)^T d + \mbox{o}(\alpha)
\end{equation}
with positive spanning set $k>0$
gaureented descent direction
for coordinate search $\kappa (G) = \frac{1}{\sqrt{n}}$
-bound $\kappa (G)$ for convergence result

Kolda Lewis Torczon (review) 2003

convergence analysis, continuously differentiable
$\bigtriangledown f$ lipschitz continuous

-geometry, positively spanning sets
(constrained case for feasible cones)

-cosine measure ( angle between steepest descent and search direction )
\begin{equation}
\kappa (G) = \min_{v \in \R{n}}   \max_{d \in G}   \frac{v^T d}{||v|| ||d||}
\end{equation}
want good search direction and
\begin{equation}
\lim \Delta_k \rightarrow 0
\end{equation}
--sufficient decrease (forcing function)
-- rational lattice ( lie on a grid)
-- moving grids (re orient grid based on previous info)

\begin{equation}
|| \grad f (x_k) || \le \kappa (G_k)^{-1} \left[ M \Delta_k \beta_{min} + \frac{\rho (\Delta_k)}{\Delta_k \beta_{min}}\right]
\end{equation}
why $\Delta_k$ is good termination criteria, the gradient is directly related

using curvature, positive definite $B_k$
\begin{equation}
d_k = - B_k^{-1} \grad f (x_k)
\end{equation}
newtons method gives
\begin{equation}
B_k = \grad^2 f(x_k)
\end{equation}
quasi-Newton just uses approximation to Hessian $\grad^2 f(x_k)$

simple decrease $f (x_{k+1} ) < f(x_k)$ is not enough to ensure convergence to stationary point
problems step length and descent direction

on Step Length, sufficient decrease Armijo 9,192, Goldstein 125, 197
\begin{align}
f(x_k + \alpha_k d_k) \le f(x_k) + c_1 \alpha_k \grad f(x_k)^T d_k  \label{step_length_1}\\
\grad f (x_k + \alpha_k d_k)^T d_k \ge c_2 \grad f(x_k)^T d_k \label{step_length_2}
\end{align}
with $0 < c_1 < c_2 < 1$
poor descent direction
\begin{equation}\label{descent_direction}
\frac{-\grad f(x_k)^T d_k}{|| \grad f (x_k) || ||d_k|| } \ge c > 0
\end{equation}


global convergence for line search
$f: \R{n} \rightarrow \R{}$,$f \in C^1$, $\grad f$ lipschit cont. with constant $M$

if $\left\{x_k\right\}$ satisfies \ref{step_length_1} \ref{step_length_2} \ref{descent_direction}
then $\lim_{k\rightarrow \infty } || \grad f(x) || = 0$

Generating Set Search
includes Torczon 257, Hookes and Jeeves 139 Multidirection 256, EVOP Box 35, Lewis Torczon 166, Coope Price 76,77 Yu 278 Lucidi Sciandrun 174 Garcia - Polomers Rodriquez 115,     (5, 13)

\begin{description}
\item[$D_k$] search direction, contains generating set $G_k$ for $\R{n}$, $H_k$ additional search direction, 
cosine measure $\kappa (G_k)$ has $\kappa_{min}$
\item[$\Delta_k$] contains parameters, $\phi_k$ expansion $\ge 1$, $\theta_k $ constracion $\in (0,1)$
\item[$\rho ( \dot )$] forcing function to ensure sufficient decrease $\rho : \left[ 0 , + \infty \right] \rightarrow \R{}$ with $\rho(t)$ decreasing as $t \rightarrow 0$, $\rho \equiv 0$ acceptable, $\frac{\rho (t)}{t} \rightarrow 0 $ as $t\rightarrow 0$ 
\end{description}

$\beta_{min}, \beta_{max}$
with $\beta_{min} \le ||d|| \le \beta_{max} $ for $\forall d \in G_k$ and $\beta_{min} \le || d ||$ for $\forall d \in H_k$


Updates
\begin{equation}
x_{k+1} = \left\{  \begin{array}{l l}
						x_k + \Delta_k d_k  	&	k \in \cS \\
						x_k					&	k \in \cU 
					\end{array}
			\right.
\end{equation}


\begin{equation}
\Delta_{k+1} = \left\{  \begin{array}{l l}
						\phi_k \Delta_k  		&	k \in \cS \\
						\theta_k \Delta_k	&	k \in \cU 
					\end{array}
			\right.
\end{equation}

Decrease condition
\begin{equation}
f(x_k  + \Delta_k d_k ) < f(x_k) - \rho ( \Delta_k))
\end{equation}

Generating Sets
$G = \left\{ d^{(1)} , ... , d^{(p)} \right\}$, with $p \ge n+1$ vectors in $\R{n}$
$G$ generates $\R{n}$ if for any $v\in \R{n}$, $\exists \lambda^{(1)}, ..., \lambda^{(p)} \ge 0$ such that
\begin{equation}
v = \sum_{i=1}^p \lambda^{(i)} d^{(i)}
\end{equation}

equivalently
$G$ generators $\R{n}$ iff $\exists d \in G$ such that $v^T d > 0$, that is there must be a descent direction, as long as $G$ is a positive spanning set.
The worst case angle from descent direction 
\begin{equation}
\kappa (G) = \min_{v \in \R{n}} \max_{d\in G} \frac{v^T d}{||v|| ||d||}
\end{equation}
$\kappa (G) > 0 $ implies $G$ is a generating set

which says $\exists d \in G$ such that
\begin{equation}
\kappa (G) || \grad f(x) || || d || \le - \grad f(x)^T d
\end{equation}

For convergence, need $\kappa (G_k) \ge \kappa_{min}$ for all $k$
coordinate directions, $D_{\oplus}$, gives $\kappa (D_\oplus ) = \frac{1}{\sqrt{n}}$


For convergences, need subsequence of unsuccessful iterates to have
\begin{equation}
\lim_{k \rightarrow + \infty} \Delta_k = 0
\end{equation}
with $k \in K \subset \cU$

Three ways to accomplish
\begin{description}
\item[sufficient decrease] Yu 278, Lucidi Sciandrone 174, Garcia Polomeros Rodriques 115
\item[rational latticies] Berman 21, 22 Cea 56 Polak 211 Torczon 257, Lewis Torczon 166
\item[moving grids] Coope and Price 77, Rosenbrock 227, Powell 213, Nelder Mead 194, (76,77,224)
\end{description}
need compact level sets?.?.?

Many Options for generating sets
$G_k \neq D_\oplus$ - 77,78,115,140,166,174,256
Simplex 164, not GSS, one search decreasing with only simple decrease

Model based direct search 71,72,74,177,220,221

explatory moves
pattern step
\begin{equation}
x_p = x_k + (x_k - x_{k-1})
\end{equation}
keep moving in same direction

without continuous differentiability, HARD TO PROVE MUCH
linearly constrained case $ Ax \le b$


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Derivative Free Optimization}
Derivative Free Optimization  (Conn, Scheinberg, Vicente) 2009  \cite{conn_2009}

analysis of variance (statistical test 203)

heuristics
simulated annealing 143
genetic, evolutionary 108 129
artifical neural network 122
particle swarm 142

coordinate search 70
Nelder Mead 177
simplex gradient 141
trust region models
interpolation, regression

Ferris Deng 92
Powell 183 conjugate directions
-Zangwill 237 modification
Toint Calliers 215
Rossenbrock 201
Frimannslund Steignberg 100 rotating directions
Custodio Vicente 70 order polling directions


solve
\begin{equation}
\min_{x \in \R{n}} f(x) 
\end{equation}
do not confuse this $f$, which represents a function from $\R{n} \rightarrow \R{}$, with the $f_{ijn}$ that represents the amount of power flowing on a branch

for our cascading model, all we require is the model to return a scalar which represents some measure of risk or cost of the blackout for a given state and be able to list potential lines in the events $h(x)$, we use probabilities with h values to find sequances and covariances which give you good information

MANY WAYS TO SOLVE
model based -polynomial -underdetermined -overdetermined
pattern search - conform to local curvature
line search
simplex gradient
simple with enforcing geometry -normalized volume
simplex hessians
trust region - cauchy step
conn scheinberg toint 59, 61	interpolation, trust region
surrogate functions


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Simulation Based Optimization
Surrogate Models??? Stochastic Krigin


Simulation Versions
%%%%%%%%
Berry Nelson papers

%\subsubsection{Optimization via Simulation}


L. Jeff Hong
Barry L. Nelson
Proceedings of the 2009 Winter Simulation Conference
A Breif Introduction to Optimization Via Simulation (OvS) \cite{hong_2009}

three types
R\&S
continuous OvS
discrete OvS

build models to design and analyze complex systems.
easy to change parameters and run what-if
find parameters that optimize system performance

\begin{equation}
\min g(x) = \Expect \left[ Y(x) \right] 	\hspace{20px} x \in \Theta \subset \R{d}
\end{equation}
$x$ is decision variable, simulation slow, only estimate of $g(x)$.  distrubtion vary over feasible region.  little known about properties of impled objective function (convexity, differentiability)
Recent comprehensive view Fu (2002)

Differences in feasibility structure of $\Theta$
\begin{itemize}
\item Ranking and Selecetion (R\&S) - $\Theta$ has small number of solution (less than 100), numerical or categorical, simulate all at select best
\item continuous OvS (COvS) -$x$ is continuous decision variable
\item discrete OvS - $x$ is discrete and integer ordered
\end{itemize}
not exhaustive but covers most problems in practice and research

academic algorithms often focus on provable properties, such as statistical claims on R\&S and convergence properties on COvS and DOvS.  in practice, Algorithms without convergence gaureentees but with good empirical performance

most commercial OvS implement robust metaheuristics
AutoStat - evolution strategies
OptQuest - scatter search, tabu search, neural networks
SimRunner2 - evolution strategies and genetic algorithms (Law 2007)

often robust and perform well on deterministic problems
lack sophisticated schemes to handle randomness in simulation outputs
misled by noise in simulation and report solutions with poor qualities


Ranking and Selection 

frequentist approach
Bayesian approach - optimal computing budget allocation (OCBA), Chen et al (2000) allocated simulation budget to maximize posterior probability of correct solution
	-expected value of information (EVI) Chick and Inoue (2001) minimize expected opportunity cost of the chosen solution

Bayesian approach has strategies to optimally allocating simulation effort to the solutions being considered

Suppose $k \ge 2$ solutions in $\Theta$, $x_1, ... x_k$.  $Y_j (x_i)$ is $j$th observation from simulating solution $x_i$.  typically assume $Y_j (x_i) \sim N(g(x_i), \sigma_i^2 )$, where $g(x_i)$ is unknown and $\sigma_i^2$ either known or unknown.  Without loss of generality, have $g(x_1) \le g(x_2) \le ... \le g(x_k)$
\begin{equation}
P[ \mbox{select } x_k | g(x_1) \le g(x_2) - \delta ] \ge 1 - \alpha
\end{equation}
$\delta > 0$ is an indifference zone (IZ), set as smallest difference that is practically significant

Bechhofer's procedure
Step 1 is to set how many samples to take, $n$, determine $h$ such that $P [ Z_i \le h, i=1,2,...,k-1 ] = 1-\alpha$ and $(Z_1, ..., Z_{k-1})$ has multivariate normal distribution with means 0, variances 1, common pairwise correlations 1/2, then
\begin{equation}
n = \left\lceil \frac{ 2h^2\sigma^2}{\delta^2} \right\rceil
\end{equation}
Step 2 take $n$ observations
Step 3 select solution with lowest sample mean $\overline{Y}_n (x_i)$

If variances are unknown, typicallly two stage procedures
variances unknown and unequal Rinotts procedure (Ronott 1978)
variances unknown and unequal and common random numbers - Nelson and Matejcik (1995)
number of solutions large, do screening step - Nelson et al (2001)

Also sequential procedures

Paulson's procedure
step 1 Let $0 < \lambda < \delta$
\begin{equation}
a = \ln \left( \frac{k-1}{\alpha} \right) \frac{\sigma^2}{\delta - \lambda}
\end{equation}
Let $I = \left\{ x_1, x_2, ..., x_k \right\}$ and $r=0$
Step 2 $r = r+1$, take one observation from each solution in $I$ and calculate $\overline{Y}_r(x_i)$
Step 3 $I_{old} = I$
\begin{equation}
I = \left\{ x_l \in I_{old} | \overline{Y}_r(x_l) \le \min_{i \in I_{old}} \overline{Y}_i(x_i) + \max \left\{ 0, a/r - \lambda \right\} \right\}
\end{equation}
Step 3 when $| I | = 1$, you have solution

fully sequential procedure, unknown and unequal variances, and crn (Kim and Nelson 2001)
non-normal and dependent (Kim and Nelson 2006)
typically require less samples
Hong and Nelson (2005) reduced number of switchings
Branke Chick Schmidt (2007) comprehensive set of experience, no R\&S procedure dominate
Bayesian procedures often more efficient in number of samples
do not provide selection guarantees that frequentist procedures do





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





\end{comment}

\theendnotes

\setcounter{endnote}{0}

