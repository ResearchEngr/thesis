\begin{comment}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% old
stochastic vs deterministic
stochastic 
-simulation based
Berry Nelson

stochastic
-opt community
Guanghai Lan, 
Sample Approximation Average
Random Gradient Descent
Robust Stochastic Approximation Nemirouski Jouditsty Lan Shapiro
Adaptive Robust Optimization for security constrained unit commitment Bertsimas Lituinov Sun Zhao Zheng
Introduction to stochastic search and optimization Spall

Stochastic versions (get stochastic book)
Sample Approximation Average
robust stochastic approximation (Nemirovsky Juditsty, Lan, Shapiro)
Spoll  -Introduction to stochastic search and Optimization
Adaptive Robust optimization for Security Constrained Unit Commitment  (Bertsimas, Lituinov, Sun, Zhao, Zheng)


\subsubsection{Pattern Search}
deterministic
-pattern search
Generating Set Search (Kolda, Lewis, Torczon Review 2003 convergence results)
derivative free optimization book (Conn Scheinberg Vicente 2003)
Simplex


\subsubsection{Model Based}
deterministic
-model based (Conn Scheinberg Vicente 2003) for all of these
interpolation
regression
trust regions


derivative free optimization
derivate used - could give examples of it not working <<< more mork
local model based


Nonlinear Optimization \endnote{\input{\mypathdfo/end-nonline}}



%%%%%%%%%%%%%%




--pattern search, using secondary simulation data, model based?, evolutionary, trust regions

Direct Search Methods: then and now ( Lewis, Torczon, Trosset ) \cite{lewis_2000}

\begin{equation}
\min_{ f: \R{n} \rightarrow \R{}} f(x)
\end{equation}
f continuously differentiable but unavailable

pattern search methods
simplex method (geometry based search strategies)
adaptive sets of search directions

some history on convergence results

Polak '71 specific algorithm converge to accumlation point $x'$ of $\left\{x_k\right\}$ such tthat 
\begin{equation}
\bigtriangledown f(x') = 0
\end{equation}

Berman provides results for fairly general algorithm based on lattice method

initialize 
$x_0, \Delta_0$
\begin{equation}
L(x_0, \Delta_0) = \left\{ x | x = x_0 + \Delta_0 \lambda, \lambda \in \Lambda \right\}
\end{equation}
where $\Lambda $ is a lattice of integral points on $\R{n}$
if $L_k  = L(x_k, \Delta_k)$, $\Delta_k = \frac{\delta_0}{\tau^k}$, $\tau > 1$ and $L_k \subset L_{k+1}$
as $k \uparrow$ , $\Delta \downarrow$, as $k$ increases, resolution increases

Cea gives convergence results on Hookes and Jeeves algorithm, for strictly convex function, you get a unique minimizer

V. Torczon generalizes Bermans results
patterns of form 
\begin{equation}
x_k' = x_k + \Delta_k B \gamma_k'
%%%x_k^' = x_k + \Delta_k \Beta \gamma_k^'
\end{equation}
where $B$ is a nonsingular matrix and $\gamma_k'$ is an integral vector
Assume that $L(x_0) = \left\{ x | f(x)  \le f(x_0) \right\} $ is compact, f cont. diff. on neighborhood of $L(x_0)$.  Then sequance of iterats $\left\{ x_k \right\}$ produced by algorithm has
\begin{equation}
\lim_{k \rightarrow \infty} \inf || \bigtriangledown f (x_k) || = 0
\end{equation}


Nelder Mead, expansion, contraction, reflection
Adaptive set of search direction ( Rosenbrock Powell)

%%%%%%%%%%%%%%%%%
Optimization by Direct Search: New Perspectives on Some Classical and Modern Methods (Kolda, Lewis, Torczon) \cite{kolda_2003}


search methods due to stochastic error 6,173,260
$f\in C^1$ - derivative is continuously differntiable, gradient method
$f \in C^2$ - twice differentiable, Newton based methods 69, 192, 197

direct search methods, noisy numerical error
issues
-slow asymptotic convergence
-problem size

derivative free 71,74, 177
-models via least squares, regression 121
-interpolation models 72, 74, 220
-no explicit model 115, 173, 174, 175

Direct Search - sequential examination of trial solutions involving comparison of trial solutions with the "best" obtained up to that time together with a strategy for determing next trial solutions (as a function of earlier results)


assume order relation
$ x \prec y$ if $f(x) < f(y)$
only finite amt of new iterates and can be enumerated

parallel implementation 93, 115, 140, 256
-load balancing 140
161,162

Hybrid approach
direct search with
-trust region 141
-evolutionary alg 131,132,133
-modeling acceleration technique 30, 31, 91, 100, 261

Smooth Unconstrained Minimization

Line search
if $f$ differntiable at $x$, $d$ is a descent direction for $f$ at $x$ 
\begin{equation}
- \grad f(x)^T d > 0
\end{equation}
and if $d$ is descent, $\alpha >0$ and sufficiently small, $x_t$ reduces value of $f$
\begin{equation}
f(x + \alpha d) = f(x) + \alpha \grad f(x)^T d + \mbox{o}(\alpha)
\end{equation}
with positive spanning set $k>0$
guareented descent direction
for coordinate search $\kappa (G) = \frac{1}{\sqrt{n}}$
-bound $\kappa (G)$ for convergence result

Kolda Lewis Torczon (review) 2003

convergence analysis, continuously differentiable
$\bigtriangledown f$ lipschitz continuous

-geometry, positively spanning sets
(constrained case for feasible cones)

-cosine measure ( angle between steepest descent and search direction )
\begin{equation}
\kappa (G) = \min_{v \in \R{n}}   \max_{d \in G}   \frac{v^T d}{||v|| ||d||}
\end{equation}
want good search direction and
\begin{equation}
\lim \Delta_k \rightarrow 0
\end{equation}
--sufficient decrease (forcing function)
-- rational lattice ( lie on a grid)
-- moving grids (re orient grid based on previous info)

\begin{equation}
|| \grad f (x_k) || \le \kappa (G_k)^{-1} \left[ M \Delta_k \beta_{min} + \frac{\rho (\Delta_k)}{\Delta_k \beta_{min}}\right]
\end{equation}
why $\Delta_k$ is good termination criteria, the gradient is directly related

using curvature, positive definite $B_k$
\begin{equation}
d_k = - B_k^{-1} \grad f (x_k)
\end{equation}
newtons method gives
\begin{equation}
B_k = \grad^2 f(x_k)
\end{equation}
quasi-Newton just uses approximation to Hessian $\grad^2 f(x_k)$

simple decrease $f (x_{k+1} ) < f(x_k)$ is not enough to ensure convergence to stationary point
problems step length and descent direction

on Step Length, sufficient decrease Armijo 9,192, Goldstein 125, 197
\begin{align}
f(x_k + \alpha_k d_k) \le f(x_k) + c_1 \alpha_k \grad f(x_k)^T d_k  \label{step_length_1}\\
\grad f (x_k + \alpha_k d_k)^T d_k \ge c_2 \grad f(x_k)^T d_k \label{step_length_2}
\end{align}
with $0 < c_1 < c_2 < 1$
poor descent direction
\begin{equation}\label{descent_direction}
\frac{-\grad f(x_k)^T d_k}{|| \grad f (x_k) || ||d_k|| } \ge c > 0
\end{equation}


global convergence for line search
$f: \R{n} \rightarrow \R{}$,$f \in C^1$, $\grad f$ lipschit cont. with constant $M$

if $\left\{x_k\right\}$ satisfies \ref{step_length_1} \ref{step_length_2} \ref{descent_direction}
then $\lim_{k\rightarrow \infty } || \grad f(x) || = 0$

Generating Set Search
includes Torczon 257, Hookes and Jeeves 139 Multidirection 256, EVOP Box 35, Lewis Torczon 166, Coope Price 76,77 Yu 278 Lucidi Sciandrun 174 Garcia - Polomers Rodriquez 115,     (5, 13)

\begin{description}
\item[$D_k$] search direction, contains generating set $G_k$ for $\R{n}$, $H_k$ additional search direction, 
cosine measure $\kappa (G_k)$ has $\kappa_{min}$
\item[$\Delta_k$] contains parameters, $\phi_k$ expansion $\ge 1$, $\theta_k $ constracion $\in (0,1)$
\item[$\rho ( \dot )$] forcing function to ensure sufficient decrease $\rho : \left[ 0 , + \infty \right] \rightarrow \R{}$ with $\rho(t)$ decreasing as $t \rightarrow 0$, $\rho \equiv 0$ acceptable, $\frac{\rho (t)}{t} \rightarrow 0 $ as $t\rightarrow 0$ 
\end{description}

$\beta_{min}, \beta_{max}$
with $\beta_{min} \le ||d|| \le \beta_{max} $ for $\forall d \in G_k$ and $\beta_{min} \le || d ||$ for $\forall d \in H_k$


Updates
\begin{equation}
x_{k+1} = \left\{  \begin{array}{l l}
						x_k + \Delta_k d_k  	&	k \in \cS \\
						x_k					&	k \in \cU 
					\end{array}
			\right.
\end{equation}


\begin{equation}
\Delta_{k+1} = \left\{  \begin{array}{l l}
						\phi_k \Delta_k  		&	k \in \cS \\
						\theta_k \Delta_k	&	k \in \cU 
					\end{array}
			\right.
\end{equation}

Decrease condition
\begin{equation}
f(x_k  + \Delta_k d_k ) < f(x_k) - \rho ( \Delta_k))
\end{equation}

Generating Sets
$G = \left\{ d^{(1)} , ... , d^{(p)} \right\}$, with $p \ge n+1$ vectors in $\R{n}$
$G$ generates $\R{n}$ if for any $v\in \R{n}$, $\exists \lambda^{(1)}, ..., \lambda^{(p)} \ge 0$ such that
\begin{equation}
v = \sum_{i=1}^p \lambda^{(i)} d^{(i)}
\end{equation}

equivalently
$G$ generators $\R{n}$ iff $\exists d \in G$ such that $v^T d > 0$, that is there must be a descent direction, as long as $G$ is a positive spanning set.
The worst case angle from descent direction 
\begin{equation}
\kappa (G) = \min_{v \in \R{n}} \max_{d\in G} \frac{v^T d}{||v|| ||d||}
\end{equation}
$\kappa (G) > 0 $ implies $G$ is a generating set

which says $\exists d \in G$ such that
\begin{equation}
\kappa (G) || \grad f(x) || || d || \le - \grad f(x)^T d
\end{equation}

For convergence, need $\kappa (G_k) \ge \kappa_{min}$ for all $k$
coordinate directions, $D_{\oplus}$, gives $\kappa (D_\oplus ) = \frac{1}{\sqrt{n}}$


For convergences, need subsequence of unsuccessful iterates to have
\begin{equation}
\lim_{k \rightarrow + \infty} \Delta_k = 0
\end{equation}
with $k \in K \subset \cU$

Three ways to accomplish
\begin{description}
\item[sufficient decrease] Yu 278, Lucidi Sciandrone 174, Garcia Polomeros Rodriques 115
\item[rational latticies] Berman 21, 22 Cea 56 Polak 211 Torczon 257, Lewis Torczon 166
\item[moving grids] Coope and Price 77, Rosenbrock 227, Powell 213, Nelder Mead 194, (76,77,224)
\end{description}
need compact level sets?.?.?

Many Options for generating sets
$G_k \neq D_\oplus$ - 77,78,115,140,166,174,256
Simplex 164, not GSS, one search decreasing with only simple decrease

Model based direct search 71,72,74,177,220,221

explatory moves
pattern step
\begin{equation}
x_p = x_k + (x_k - x_{k-1})
\end{equation}
keep moving in same direction

without continuous differentiability, HARD TO PROVE MUCH
linearly constrained case $ Ax \le b$


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Derivative Free Optimization}
Derivative Free Optimization  (Conn, Scheinberg, Vicente) 2009  \cite{conn_2009}

analysis of variance (statistical test 203)

heuristics
simulated annealing 143
genetic, evolutionary 108 129
artifical neural network 122
particle swarm 142

coordinate search 70
Nelder Mead 177
simplex gradient 141
trust region models
interpolation, regression

Ferris Deng 92
Powell 183 conjugate directions
-Zangwill 237 modification
Toint Calliers 215
Rossenbrock 201
Frimannslund Steignberg 100 rotating directions
Custodio Vicente 70 order polling directions


solve
\begin{equation}
\min_{x \in \R{n}} f(x) 
\end{equation}
do not confuse this $f$, which represents a function from $\R{n} \rightarrow \R{}$, with the $f_{ijn}$ that represents the amount of power flowing on a branch

for our cascading model, all we require is the model to return a scalar which represents some measure of risk or cost of the blackout for a given state and be able to list potential lines in the events $h(x)$, we use probabilities with h values to find sequances and covariances which give you good information

MANY WAYS TO SOLVE
model based -polynomial -underdetermined -overdetermined
pattern search - conform to local curvature
line search
simplex gradient
simple with enforcing geometry -normalized volume
simplex hessians
trust region - cauchy step
conn scheinberg toint 59, 61	interpolation, trust region
surrogate functions


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Simulation Based Optimization
Surrogate Models??? Stochastic Krigin


Simulation Versions
%%%%%%%%
Berry Nelson papers

%\subsubsection{Optimization via Simulation}


L. Jeff Hong
Barry L. Nelson
Proceedings of the 2009 Winter Simulation Conference
A Breif Introduction to Optimization Via Simulation (OvS) \cite{hong_2009}

three types
R\&S
continuous OvS
discrete OvS

build models to design and analyze complex systems.
easy to change parameters and run what-if
find parameters that optimize system performance

\begin{equation}
\min g(x) = \Expect \left[ Y(x) \right] 	\hspace{20px} x \in \Theta \subset \R{d}
\end{equation}
$x$ is decision variable, simulation slow, only estimate of $g(x)$.  distrubtion vary over feasible region.  little known about properties of impled objective function (convexity, differentiability)
Recent comprehensive view Fu (2002)

Differences in feasibility structure of $\Theta$
\begin{itemize}
\item Ranking and Selecetion (R\&S) - $\Theta$ has small number of solution (less than 100), numerical or categorical, simulate all at select best
\item continuous OvS (COvS) -$x$ is continuous decision variable
\item discrete OvS - $x$ is discrete and integer ordered
\end{itemize}
not exhaustive but covers most problems in practice and research

academic algorithms often focus on provable properties, such as statistical claims on R\&S and convergence properties on COvS and DOvS.  in practice, Algorithms without convergence guarantees but with good empirical performance

most commercial OvS implement robust metaheuristics
AutoStat - evolution strategies
OptQuest - scatter search, tabu search, neural networks
SimRunner2 - evolution strategies and genetic algorithms (Law 2007)

often robust and perform well on deterministic problems
lack sophisticated schemes to handle randomness in simulation outputs
misled by noise in simulation and report solutions with poor qualities


Ranking and Selection 

frequentist approach
Bayesian approach - optimal computing budget allocation (OCBA), Chen et al (2000) allocated simulation budget to maximize posterior probability of correct solution
	-expected value of information (EVI) Chick and Inoue (2001) minimize expected opportunity cost of the chosen solution

Bayesian approach has strategies to optimally allocating simulation effort to the solutions being considered

Suppose $k \ge 2$ solutions in $\Theta$, $x_1, ... x_k$.  $Y_j (x_i)$ is $j$th observation from simulating solution $x_i$.  typically assume $Y_j (x_i) \sim N(g(x_i), \sigma_i^2 )$, where $g(x_i)$ is unknown and $\sigma_i^2$ either known or unknown.  Without loss of generality, have $g(x_1) \le g(x_2) \le ... \le g(x_k)$
\begin{equation}
P[ \mbox{select } x_k | g(x_1) \le g(x_2) - \delta ] \ge 1 - \alpha
\end{equation}
$\delta > 0$ is an indifference zone (IZ), set as smallest difference that is practically significant

Bechhofer's procedure
Step 1 is to set how many samples to take, $n$, determine $h$ such that $P [ Z_i \le h, i=1,2,...,k-1 ] = 1-\alpha$ and $(Z_1, ..., Z_{k-1})$ has multivariate normal distribution with means 0, variances 1, common pairwise correlations 1/2, then
\begin{equation}
n = \left\lceil \frac{ 2h^2\sigma^2}{\delta^2} \right\rceil
\end{equation}
Step 2 take $n$ observations
Step 3 select solution with lowest sample mean $\overline{Y}_n (x_i)$

If variances are unknown, typicallly two stage procedures
variances unknown and unequal Rinotts procedure (Ronott 1978)
variances unknown and unequal and common random numbers - Nelson and Matejcik (1995)
number of solutions large, do screening step - Nelson et al (2001)

Also sequential procedures

Paulson's procedure
step 1 Let $0 < \lambda < \delta$
\begin{equation}
a = \ln \left( \frac{k-1}{\alpha} \right) \frac{\sigma^2}{\delta - \lambda}
\end{equation}
Let $I = \left\{ x_1, x_2, ..., x_k \right\}$ and $r=0$
Step 2 $r = r+1$, take one observation from each solution in $I$ and calculate $\overline{Y}_r(x_i)$
Step 3 $I_{old} = I$
\begin{equation}
I = \left\{ x_l \in I_{old} | \overline{Y}_r(x_l) \le \min_{i \in I_{old}} \overline{Y}_i(x_i) + \max \left\{ 0, a/r - \lambda \right\} \right\}
\end{equation}
Step 3 when $| I | = 1$, you have solution

fully sequential procedure, unknown and unequal variances, and crn (Kim and Nelson 2001)
non-normal and dependent (Kim and Nelson 2006)
typically require less samples
Hong and Nelson (2005) reduced number of switchings
Branke Chick Schmidt (2007) comprehensive set of experience, no R\&S procedure dominate
Bayesian procedures often more efficient in number of samples
do not provide selection guarantees that frequentist procedures do





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





\end{comment}


