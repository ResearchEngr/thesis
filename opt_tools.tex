\section{Optimization Tools}

Want to do design problems on power grids.  Optimize these models. 

what characteristics does this model have

not convex, not continuously differentiable, noisy 

need robust solution methodology 


stochastic vs deterministic

stochastic 
-simulation based
Berry Nelson

stochastic
-opt community
Guanghai Lan, 
Sample Approximation Average
Random Gradient Descent
Robust Stochastic Approximation Nemirouski Jouditsty Lan Shapiro
Adaptive Robust Optimization for security constrained unit commitment Bertsimas Lituinov Sun Zhao Zheng
Introduction to stochastic search and optimization Spall

deterministic
-pattern search
Generating Set Search (Kolda, Lewis, Torczon Review 2003 convergence results)
derivative free optimization book (Conn Scheinberg Vicente 2003)
Simplex

deterministic
-model based (Conn Scheinberg Vicente 2003) for all of these
interpolation
regression
trust regions


derivative free optimization
derivate used - could give examples of it not working <<< more mork
local model based

\subsection{Stochastic Programming}

\subsubsection{Logical Constraints}

\subsubsection{Multi-Stage Stochastic Programming}
tree details


\subsection{Monte Carlo Simulation}

\subsubsection{Linear Programming}
DC POWER flow

\subsubsection{Nonlinear Optimization}
Newton rhapson
\ref{ac-pf} are static power flow equations
Each bus characterized by $P_i, Q_i, \delta_i, |V_i|$
$4 n$ variables, supply $2 n$, find remaining $2 n -1$ using iterative numerical techniques and a slack bus for unique solution
gauss-seidel
newton raphson  look for $F(x) = 0$, approximate by tangent hyper plane, solved for correction $\Delta x$
\begin{equation}
\left[ \begin{array}{ c } \Delta P \\ \Delta Q \end{array}	\right]
=
\left[ \begin{array}{ c c } J_{P \theta} J_{PV} \\ J_{Q \theta} J_{QV} \end{array}	\right]
\left[ \begin{array}{ c } \Delta \theta \\ \Delta V \end{array}	\right]
=
J \left[ \begin{array}{ c } \Delta \theta \\ \Delta V \end{array}	\right]
\end{equation}

fast decoupled load flow (approximation of newton rhapson technique)
assume that $J_{PV}$ and $J_{Q\theta}$ are zero, that is, real power flow not sensitive to voltage changes and reactive power not sensitive to voltage angle changes


\subsubsection{Variance Reduction}
common random number
reference textbook on simulation Simulation Modeling and Analysis (Averill M. Law) \cite{law_2007}
Dobson splitting method, type of importance sampling

Covariance matrix
try instead of solving economic dispatch, solve min variance problem ??? analogous to CAPM, need to make modifcation though!!!! its all related sooo close


look for a set of net power injects wich minimizes the variance in revenue/cost at all of the nodes, perhaps weighted by centrallity, otherwise weight evenly?
what are characterstics of this set versus the one formed from economic dispatch over day ahead and real time markets
what is difference in cost
what is difference in reliability


%%%%%%%%%%%%%%


\subsection{Derivative Free Optimization}

\subsubsection{Pattern Search}

\subsubsection{Model Based}

\subsubsection{Stochastic Approximation}


--pattern search, using secondary simulation data, model based?, evolutionary, trust regions
Direct Search Methods: then and now ( Lewis, Torczon, Trosset ) \cite{lewis_2000}

\begin{equation}
\min_{ f: \R{n} \rightarrow \R{}} f(x)
\end{equation}
f continuously differentiable but unavailable

pattern search methods
simplex method (geometry based search strategies)
adaptive sets of search directions

some history on convergence results

Polak '71 specific algorithm converge to accumlation point $x'$ of $\left\{x_k\right\}$ such tthat 
\begin{equation}
\bigtriangledown f(x') = 0
\end{equation}

Berman provides results for fairly general algorithm based on lattice method

initialize 
$x_0, \Delta_0$
\begin{equation}
L(x_0, \Delta_0) = \left\{ x | x = x_0 + \Delta_0 \lambda, \lambda \in \Lambda \right\}
\end{equation}
where $\Lambda $ is a lattice of integral points on $\R{n}$
if $L_k  = L(x_k, \Delta_k)$, $\Delta_k = \frac{\delta_0}{\tau^k}$, $\tau > 1$ and $L_k \subset L_{k+1}$
as $k \uparrow$ , $\Delta \downarrow$, as $k$ increases, resolution increases

Cea gives convergence results on Hookes and Jeeves algorithm, for strictly convex function, you get a unique minimizer

V. Torczon generalizes Bermans results
patterns of form 
\begin{equation}
x_k' = x_k + \Delta_k B \gamma_k'
%%%x_k^' = x_k + \Delta_k \Beta \gamma_k^'
\end{equation}
where $B$ is a nonsingular matrix and $\gamma_k'$ is an integral vector
Assume that $L(x_0) = \left\{ x | f(x)  \le f(x_0) \right\} $ is compact, f cont. diff. on neighborhood of $L(x_0)$.  Then sequance of iterats $\left\{ x_k \right\}$ produced by algorithm has
\begin{equation}
\lim_{k \rightarrow \infty} \inf || \bigtriangledown f (x_k) || = 0
\end{equation}


Nelder Mead, expansion, contraction, reflection
Adaptive set of search direction ( Rosenbrock Powell)

%%%%%%%%%%%%%%%%%
Optimization by Direct Search: New Perspectives on Some Classical and Modern Methods (Kolda, Lewis, Torczon) \cite{kolda_2003}


search methods due to stochastic error 6,173,260
$f\in C^1$ - derivative is continuously differntiable, gradient method
$f \in C^2$ - twice differentiable, Newton based methods 69, 192, 197

direct search methods, noisy numerical error
issues
-slow asymptotic convergence
-problem size

derivative free 71,74, 177
-models via least squares, regression 121
-interpolation models 72, 74, 220
-no explicit model 115, 173, 174, 175

Direct Search - sequential examination of trial solutions involving comparison of trial solutions with the "best" obtained up to that time together with a strategy for determing next trial solutions (as a function of earlier results)

assume order relation
$ x \prec y$ if $f(x) < f(y)$
only finite amt of new iterates and can be enumerated

parallel implementation 93, 115, 140, 256
-load balancing 140
161,162

Hybrid approach
direct search with
-trust region 141
-evolutionary alg 131,132,133
-modeling acceleration technique 30, 31, 91, 100, 261

Smooth Unconstrained Minimization

Line search
if $f$ differntiable at $x$, $d$ is a descent direction for $f$ at $x$ 
\begin{equation}
- \grad f(x)^T d > 0
\end{equation}
and if $d$ is descent, $\alpha >0$ and sufficiently small, $x_t$ reduces value of $f$
\begin{equation}
f(x + \alpha d) = f(x) + \alpha \grad f(x)^T d + \mbox{o}(\alpha)
\end{equation}
with positive spanning set $k>0$
gaureented descent direction
for coordinate search $\kappa (G) = \frac{1}{\sqrt{n}}$
-bound $\kappa (G)$ for convergence result

Kolda Lewis Torczon (review) 2003
convergence analysis, continuously differentiable
$\bigtriangledown f$ lipschitz continuous

-geometry, positively spanning sets
(constrained case for feasible cones)

-cosine measure ( angle between steepest descent and search direction )
\begin{equation}
\kappa (G) = \min_{v \in \R{n}}   \max_{d \in G}   \frac{v^T d}{||v|| ||d||}
\end{equation}
want good search direction and
\begin{equation}
\lim \Delta_k \rightarrow 0
\end{equation}
--sufficient decrease (forcing function)
-- rational lattice ( lie on a grid)
-- moving grids (re orient grid based on previous info)

\begin{equation}
|| \grad f (x_k) || \le \kappa (G_k)^{-1} \left[ M \Delta_k \beta_{min} + \frac{\rho (\Delta_k)}{\Delta_k \beta_{min}}\right]
\end{equation}
why $\Delta_k$ is good termination criteria, the gradient is directly related

using curvature, positive definite $B_k$
\begin{equation}
d_k = - B_k^{-1} \grad f (x_k)
\end{equation}
newtons method gives
\begin{equation}
B_k = \grad^2 f(x_k)
\end{equation}
quasi-Newton just uses approximation to Hessian $\grad^2 f(x_k)$

simple decrease $f (x_{k+1} ) < f(x_k)$ is not enough to ensure convergence to stationary point
problems step length and descent direction

on Step Length, sufficient decrease Armijo 9,192, Goldstein 125, 197
\begin{align}
f(x_k + \alpha_k d_k) \le f(x_k) + c_1 \alpha_k \grad f(x_k)^T d_k  \label{step_length_1}\\
\grad f (x_k + \alpha_k d_k)^T d_k \ge c_2 \grad f(x_k)^T d_k \label{step_length_2}
\end{align}
with $0 < c_1 < c_2 < 1$
poor descent direction
\begin{equation}\label{descent_direction}
\frac{-\grad f(x_k)^T d_k}{|| \grad f (x_k) || ||d_k|| } \ge c > 0
\end{equation}


global convergence for line search
$f: \R{n} \rightarrow \R{}$,$f \in C^1$, $\grad f$ lipschit cont. with constant $M$

if $\left\{x_k\right\}$ satisfies \ref{step_length_1} \ref{step_length_2} \ref{descent_direction}
then $\lim_{k\rightarrow \infty } || \grad f(x) || = 0$

Generating Set Search
includes Torczon 257, Hookes and Jeeves 139 Multidirection 256, EVOP Box 35, Lewis Torczon 166, Coope Price 76,77 Yu 278 Lucidi Sciandrun 174 Garcia - Polomers Rodriquez 115,     (5, 13)

\begin{description}
\item[$D_k$] search direction, contains generating set $G_k$ for $\R{n}$, $H_k$ additional search direction, 
cosine measure $\kappa (G_k)$ has $\kappa_{min}$
\item[$\Delta_k$] contains parameters, $\phi_k$ expansion $\ge 1$, $\theta_k $ constracion $\in (0,1)$
\item[$\rho ( \dot )$] forcing function to ensure sufficient decrease $\rho : \left[ 0 , + \infty \right] \rightarrow \R{}$ with $\rho(t)$ decreasing as $t \rightarrow 0$, $\rho \equiv 0$ acceptable, $\frac{\rho (t)}{t} \rightarrow 0 $ as $t\rightarrow 0$ 
\end{description}

$\beta_{min}, \beta_{max}$
with $\beta_{min} \le ||d|| \le \beta_{max} $ for $\forall d \in G_k$ and $\beta_{min} \le || d ||$ for $\forall d \in H_k$

Updates
\begin{equation}
x_{k+1} = \left\{  \begin{array}{l l}
						x_k + \Delta_k d_k  	&	k \in \cS \\
						x_k					&	k \in \cU 
					\end{array}
			\right.
\end{equation}


\begin{equation}
\Delta_{k+1} = \left\{  \begin{array}{l l}
						\phi_k \Delta_k  		&	k \in \cS \\
						\theta_k \Delta_k	&	k \in \cU 
					\end{array}
			\right.
\end{equation}

Decrease condition
\begin{equation}
f(x_k  + \Delta_k d_k ) < f(x_k) - \rho ( \Delta_k))
\end{equation}

Generating Sets
$G = \left\{ d^{(1)} , ... , d^{(p)} \right\}$, with $p \ge n+1$ vectors in $\R{n}$
$G$ generates $\R{n}$ if for any $v\in \R{n}$, $\exists \lambda^{(1)}, ..., \lambda^{(p)} \ge 0$ such that
\begin{equation}
v = \sum_{i=1}^p \lambda^{(i)} d^{(i)}
\end{equation}

equivalently
$G$ generators $\R{n}$ iff $\exists d \in G$ such that $v^T d > 0$, that is there must be a descent direction, as long as $G$ is a positive spanning set.
The worst case angle from descent direction 
\begin{equation}
\kappa (G) = \min_{v \in \R{n}} \max_{d\in G} \frac{v^T d}{||v|| ||d||}
\end{equation}
$\kappa (G) > 0 $ implies $G$ is a generating set

which says $\exists d \in G$ such that
\begin{equation}
\kappa (G) || \grad f(x) || || d || \le - \grad f(x)^T d
\end{equation}

For convergence, need $\kappa (G_k) \ge \kappa_{min}$ for all $k$
coordinate directions, $D_{\oplus}$, gives $\kappa (D_\oplus ) = \frac{1}{\sqrt{n}}$


For convergences, need subsequence of unsuccessful iterates to have
\begin{equation}
\lim_{k \rightarrow + \infty} \Delta_k = 0
\end{equation}
with $k \in K \subset \cU$

Three ways to accomplish
\begin{description}
\item[sufficient decrease] Yu 278, Lucidi Sciandrone 174, Garcia Polomeros Rodriques 115
\item[rational latticies] Berman 21, 22 Cea 56 Polak 211 Torczon 257, Lewis Torczon 166
\item[moving grids] Coope and Price 77, Rosenbrock 227, Powell 213, Nelder Mead 194, (76,77,224)
\end{description}
need compact level sets?.?.?

Many Options for generating sets
$G_k \neq D_\oplus$ - 77,78,115,140,166,174,256
Simplex 164, not GSS, one search decreasing with only simple decrease

Model based direct search 71,72,74,177,220,221

explatory moves
pattern step
\begin{equation}
x_p = x_k + (x_k - x_{k-1})
\end{equation}
keep moving in same direction

without continuous differentiability, HARD TO PROVE MUCH
linearly constrained case $ Ax \le b$


%%%%%%%%%%%%%%%


Model Based
Simplex Based  


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Derivative Free Optimization  (Conn, Scheinberg, Vicente) 2009  \cite{conn_2009}

analysis of variance (statistical test 203)

heuristics
simulated annealing 143
genetic, evolutionary 108 129
artifical neural network 122
particle swarm 142

coordinate search 70
Nelder Mead 177
simplex gradient 141
trust region models
interpolation, regression

Ferris Deng 92
Powell 183 conjugate directions
-Zangwill 237 modification
Toint Calliers 215
Rossenbrock 201
Frimannslund Steignberg 100 rotating directions
Custodio Vicente 70 order polling directions


solve
\begin{equation}
\min_{x \in \R{n}} f(x) 
\end{equation}
do not confuse this $f$, which represents a function from $\R{n} \rightarrow \R{}$, with the $f_{ijn}$ that represents the amount of power flowing on a branch

for our cascading model, all we require is the model to return a scalar which represents some measure of risk or cost of the blackout for a given state and be able to list potential lines in the events $h(x)$, we use probabilities with h values to find sequances and covariances which give you good information

MANY WAYS TO SOLVE
model based -polynomial -underdetermined -overdetermined
pattern search - conform to local curvature
line search
simplex gradient
simple with enforcing geometry -normalized volume
simplex hessians
trust region - cauchy step
conn scheinberg toint 59, 61	interpolation, trust region
surrogate functions


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Simulation Based Optimization
Surrogate Models??? Stochastic Krigin


Simulation Versions
%%%%%%%%
Berry Nelson papers

\subsubsection{Optimization via Simulation}


L. Jeff Hong
Barry L. Nelson
Proceedings of the 2009 Winter Simulation Conference
A Breif Introduction to Optimization Via Simulation (OvS) \cite{hong_2009}

three types
R\&S
continuous OvS
discrete OvS

build models to design and analyze complex systems.
easy to change parameters and run what-if
find parameters that optimize system performance

\begin{equation}
\min g(x) = \Expect \left[ Y(x) \right] 	\hspace{20px} x \in \Theta \subset \R{d}
\end{equation}
$x$ is decision variable, simulation slow, only estimate of $g(x)$.  distrubtion vary over feasible region.  little known about properties of impled objective function (convexity, differentiability)
Recent comprehensive view Fu (2002)

Differences in feasibility structure of $\Theta$
\begin{itemize}
\item Ranking and Selecetion (R\&S) - $\Theta$ has small number of solution (less than 100), numerical or categorical, simulate all at select best
\item continuous OvS (COvS) -$x$ is continuous decision variable
\item discrete OvS - $x$ is discrete and integer ordered
\end{itemize}
not exhaustive but covers most problems in practice and research

academic algorithms often focus on provable properties, such as statistical claims on R\&S and convergence properties on COvS and DOvS.  in practice, Algorithms without convergence gaureentees but with good empirical performance

most commercial OvS implement robust metaheuristics
AutoStat - evolution strategies
OptQuest - scatter search, tabu search, neural networks
SimRunner2 - evolution strategies and genetic algorithms (Law 2007)

often robust and perform well on deterministic problems
lack sophisticated schemes to handle randomness in simulation outputs
misled by noise in simulation and report solutions with poor qualities


Ranking and Selection 

frequentist approach
Bayesian approach - optimal computing budget allocation (OCBA), Chen et al (2000) allocated simulation budget to maximize posterior probability of correct solution
	-expected value of information (EVI) Chick and Inoue (2001) minimize expected opportunity cost of the chosen solution

Bayesian approach has strategies to optimally allocating simulation effort to the solutions being considered

Suppose $k \ge 2$ solutions in $\Theta$, $x_1, ... x_k$.  $Y_j (x_i)$ is $j$th observation from simulating solution $x_i$.  typically assume $Y_j (x_i) \sim N(g(x_i), \sigma_i^2 )$, where $g(x_i)$ is unknown and $\sigma_i^2$ either known or unknown.  Without loss of generality, have $g(x_1) \le g(x_2) \le ... \le g(x_k)$
\begin{equation}
P[ \mbox{select } x_k | g(x_1) \le g(x_2) - \delta ] \ge 1 - \alpha
\end{equation}
$\delta > 0$ is an indifference zone (IZ), set as smallest difference that is practically significant

Bechhofer's procedure
Step 1 is to set how many samples to take, $n$, determine $h$ such that $P [ Z_i \le h, i=1,2,...,k-1 ] = 1-\alpha$ and $(Z_1, ..., Z_{k-1})$ has multivariate normal distribution with means 0, variances 1, common pairwise correlations 1/2, then
\begin{equation}
n = \left\lceil \frac{ 2h^2\sigma^2}{\delta^2} \right\rceil
\end{equation}
Step 2 take $n$ observations
Step 3 select solution with lowest sample mean $\overline{Y}_n (x_i)$

If variances are unknown, typicallly two stage procedures
variances unknown and unequal Rinotts procedure (Ronott 1978)
variances unknown and unequal and common random numbers - Nelson and Matejcik (1995)
number of solutions large, do screening step - Nelson et al (2001)

Also sequential procedures

Paulson's procedure
step 1 Let $0 < \lambda < \delta$
\begin{equation}
a = \ln \left( \frac{k-1}{\alpha} \right) \frac{\sigma^2}{\delta - \lambda}
\end{equation}
Let $I = \left\{ x_1, x_2, ..., x_k \right\}$ and $r=0$
Step 2 $r = r+1$, take one observation from each solution in $I$ and calculate $\overline{Y}_r(x_i)$
Step 3 $I_{old} = I$
\begin{equation}
I = \left\{ x_l \in I_{old} | \overline{Y}_r(x_l) \le \min_{i \in I_{old}} \overline{Y}_i(x_i) + \max \left\{ 0, a/r - \lambda \right\} \right\}
\end{equation}
Step 3 when $| I | = 1$, you have solution

fully sequential procedure, unknown and unequal variances, and crn (Kim and Nelson 2001)
non-normal and dependent (Kim and Nelson 2006)
typically require less samples
Hong and Nelson (2005) reduced number of switchings
Branke Chick Schmidt (2007) comprehensive set of experience, no R\&S procedure dominate
Bayesian procedures often more efficient in number of samples
do not provide selection guarantees that frequentist procedures do



\subsubsection{Stochastic Approximation}

Stochastic versions (get stochastic book)
Sample Approximation Average
robust stochastic approximation (Nemirovsky Juditsty, Lan, Shapiro)
Spoll  -Introduction to stochastic search and Optimization
Adaptive Robust optimization for Security Constrained Unit Commitment  (Bertsimas, Lituinov, Sun, Zhao, Zheng)


%%%%%%%%%%%%%%%%%
Lan -  Randomized Stochastic Gradient
stochastic descent
zeroth order function values only
gaussian smoothhing function  (nesterov)

-mirror descent Meirovski 23,18
-sample average approximation 16, 36
-This
-subgradient averaging 13, 15, 26

\begin{equation}
f^* := \inf_{x \in \R{n}} f(x)
\end{equation}
$f: \R{n} \rightarrow \R{}$ differentiable
$\bigtriangledown$ Lipschitz continuous
\begin{equation}
|| \bigtriangledown f(y) - \grad f (x) || \le L || y - x ||
\end{equation}

stochastic gradient
$G (x_k, \xi_k$, $\xi_k$ random variable, distribution $P_k$ supported on $\Xi \subset \R{d}$
with following assumptions, get convergence results that are good
\begin{align}
\Expect \left[ G(x_k,\xi_k) \right] = \grad f (x_k)		\\
\Expect \left[ || G(x_k,\xi_k) - \grad f(x)||^2 \right] \le \sigma^2
\end{align}

get $(\epsilon, \Lambda)$ solution, $\epsilon > 0$, $\Lambda \in (0,1)$
st.
Prob $\left\{ || \grad f(\overline{x}) ||^2 \le \epsilon \right\} \ge 1-\Lambda$
is bounded

%%%%%%%%%%%%%%%%%%%


%\subsubsection{Stochastic Programming}

%%%%%%%%%%%%%%%%%%%%
Stochastic Trust Region	(Chang and Wan) \cite{chang_2009}
\begin{equation}
\min_{x \in \mathbb{X}^p} g(x)
\end{equation}
with $g(x) = \Expect [ G(x,\omega) ]$, 
$G(x, \omega)$ is stochastic response
$\omega$ stochastic effect of system
$x$ is controllable parameter
\begin{equation}
G(x, \omega) = g(x) + \epsilon_x
\end{equation}
$g(x)$ uknown deterministic function
$\epsilon_x$ is random error induced by $\omega$, assume $\epsilon_x \sim F( \dot )$ where 
$F( \dot )$ is a general distribution, mean zero and variance $\sigma_x^2$

Assumptions
$G$ expectation bounded above
$g$ bounded below, twice differentiable, gradient and Hessian uniformly bounded
for a sufficiently small neighborhood, $G$ can be modeled as a quadratic with error $\epsilon_x$ 
--Taylor Teorem (Trench 2003)

Strong Law of Large Numbers
\begin{equation}
\overline{G}_N (x) = \frac{ \sum_{i=1}^N G(x,\omega_i) }{N} \rightarrow g(x) \mbox{    w.p. 1}
\end{equation}
as $N \rightarrow \infty$ for every $x \in \mathbb{X}^p$

Uses 2 stage process
initially, just use first order points
as trust region shrinks, there is transition points
begin to sample more, construct second order model, and reduce trust region at successful iterates

%%%%%%%%%%%%%%%%%%%%%


reference textbook on stochastic programing
Lectures on Stochastic Programing (Shapiro) \cite{shapiro_2009}


Master Problem

Stochastic Approximation and Gradient Estimation

\begin{equation}
x_{n+1} = \prod_\Theta \left[ x_n - a_n \hat{\grad} g(x_n) \right]
\end{equation}
$x_n$ solution at iteration $n$, $\hat\grad g(x_n)$ is estimate of gradient, $\lb a_n \rb$ sequance of positive real numbers called gain sequence, and $\prod_\Theta$ is projection onto feasibly region.

SA alg first proposed by Robbins and Monro (1951) - root finding algorithm
stochastic version of steepest descent algorithm
but gradient in SA is noisy estimate, convergence requires $\Expect \left[ \hat \grad g(x_n) \right] - g(x_n) \rightarrow 0$ at a certain rate
step size is often pre-determined, instead of adaptive
convergence requires $\sum_{n=1}^\infty a_n = \infty$ and $\sum_{n=1}^\infty a_n^2 < \infty$, algorithm sensitive to choice of $\lb a_n \rb$.  $\lb a_n \rb $ too large and $x_n$ oscillatory, $\lb a_n \rb $ too small, $x_n$ barely changes

simulation as black box $\grad g(x)$ by finite difference, use forward estimate with $d+1$ sim evals or central difference with $2d$ sim runs.
when dimension of $x$ large, not efficient due to number of sims required for one gradient evaluation
Spall (1992) proposed simultaneuous pertubation uses 2 sim runs to produce estimate
Spall (1998) gives nice intro to theory and application

When inside structure known, use this to get better gradient estimators
Fu (2008) gives excellent overview, pertubation analysis (PA), likelihood ratio/score function (LR/SF)
PA proposed by Ho and Cao (1983)
\begin{equation}
\grad g(x) = \grad \Expect [ Y(x) ] = \Expect [ \grad Y (x) ] 
\end{equation}
with $Y(x)$ stochastically lipschitz continuous and differentiable with prob 1.
overcome bad properties of $Y(x)$
smoothed pertubabtion analysis Gong and Ho (1987) Fu and Hu (1997)
pathwise method Hong (2009) Hong and Liu (2009)

LR/SF method in Reiman Weiss (1989) Glynn (1990)
represent $Y(x) = h(Z)$, $Z$ is vector of random variables generated in sim and $h(Z)$ is performance measure intereseted in .  $f_z(z,x)$ denote probability density of $Z$.  Note $x$ is only in density function of $Z$ and not $h(\dot)$.
then under some regularity conditoins
\begin{align}
\grad g(x) &= \grad \Expect [ h(Z) ] = \grad \int h(z) f_z(z,x) dz = \int h(z) \grad_x f_z(z,x) dz \\
	&= \Expect [ h(Z) \grad_x \log [ f_z(Z,x) ] ]
\end{align}
if $f_z(z,x)$ is known, then LR SF estimate $\grad g(x)$ by $h(Z) \grad_x \log [ f_z(Z,x) ]$. requires only single simulation run to comput, is unbiased estimaor, and does not require $Y(x)$ to be stochastic lipschitz continuous, applicabiliity often wider than PA, however variance of estimator is often high

Random Search Algorithms




\subsection{Variational Inequalities and Equillibrium}
%\subsection{Market Model}



\subsubsection{Optimality Conditions}
KKT conditions
pricing
