\newcommand{\mypath}{../thesis}
\chapter{Introduction}
This thesis focuses on reliability issues for the electricity grid that powers the United States.  Electricity is a critical service used by almost every person and company within our country.  Reliability issues cost industry billions of dollars every year.  Large scale power outages are national disasters due to the loss of services such as cell phones, city wide water pumping, gasoline stations, trains, subways, and cooling which inevitably lead to economic loss and loss of life.  

The introduction starts with an overview of the electrical infrastructure of the United States.  Following are the basics of how the power system operates on a day to day basis and the organizational structure that operates it. Then, the events of August 2003 are explained where part of the Northeast electricity grid collapsed in a cascading power failure leading to millions of people without power, billions in economic loss and the loss of life.  This is just one example of a large scale power outage, which is rare, but extremely costly to society. After this, general reliability issues of the power grid will be discussed which cost society billions annually.
 
After defining the problem, this thesis explains the tools and metholodgy for attempting to make our system more robust against such failures.  First, a model of the cascading process is made.  This model captures the effects of the cascading process while remaining simple enough to solve in a reasonable time.  After a model is developed, design problems are formulated in order to improve either the infrastructure or the operation of the system.  These design problems fit into broad families of optimization problems.  Algorithms for these types of problems are used and modified for our particular problem.  Finally, with the help of computation resources, solutions are found to these design problems in order to gain insight into the problem and help protect our system from potential future outages.

\section{Power Systems Introduction}
\input{\mypath/intro/power_system_intro}

\section{Cascading Power Failures}
A cascading power failure is a set of failures of individual components of the transmission system which leads to the redistribution of power over the new topology.
 When the system is in a stable operating position, individual components will often have a negligable effect on system wide distribution.  However, after several failures, the transmission network may not have enough capacity to distribute the power from the generation to the load.  This can cause a series of fast acting trips in which a large portion of the grid may fail.  While rare, these events are extremely costly.

\input{\mypath/intro/ne_2003} 

\section{Reliability in Power Systems}
The electricty grid is held to a higher reliability standard than other services since it is critical infrastructure for society.  Even though the power system maintains a very high level of reliability, power interuptions still have large costs to consumers.  After a discussion of the economic costs, the trends in power outages for the North American grid will be looked at.

The economic loss caused by power interruptions to electricity consumers in the United States for 2001 was \$79 billion.\cite{lacommare_2006}  This can be compared to the total revenue of electrical sales in the year 2001 of \$247 billion.  The single event of the Northeast blackout in 2003 was estimated to have a total impact on US workers, consumers, and taxpayers as a loss of approximately \$6.4 billion.\cite{anderson_2003}  This cost is hidden from the system, but may entertain the notion that the true cost of electricity is higher than the current prices.  

The current power system has evolved throughout the last century due to economic and reliability issues and the responses of the operating entities to these forces.  The power system has self-organized into a dynamic equilibrium where blackouts of all sizes occur \cite{dobson_2001}.  The average frequency of blackouts in the United States is 13 days and has been the same for 30 years \cite{carreras_2004}, which appears to represent this equillibrium.  
	
NERC data shows that distribution of blackouts for the last 15 years (1988-2003) follows a power tail curve with an exponent of around $-1.3\pm.2$ \cite{carreras_2004}. This is strengthed further by Hines et. al. in 2008.  NERC data from 1984-2006 shows that the frequency of blackouts in the United States is not decreasing, changes seasonally and with the time of day, and follows a power-law distribution.\cite{hines_2008} \cite{hines_2009}

Some emerging conditions on the grid may make protection more important and more difficult \ref{tab:change1}.  Electric vehicles adopted in masse have a potential to offer useful services to the power grid through the use of a sizeable amount of energy storage in aggregate.  However, they also represent a considerable stress on the system in different spots and ways than it is used to.  The ideal car battery system would have a quick charge, that is, the ability to transfer the energy from the grid to the battery extremely quickly.  This would allow consumers to use charging stations similar to the current gasoline stations for combustion engines.  A charging station capable of charging many cars would have extremely high and unpredictable volatility that the system will need to protect against.

The increased penetration of renewable energy production also increases stress on the power system.  Wind and solar are both variable generation devices which do not actively control the amount of electricity being produced.  The system needs to maintain ample ramping capability in order to maintain stability.  These stresses can be reduced by improving the quality of forecasting efforts on various timescales.  In addition, these technologies are geographically constrained by their fuel source availiability, wind speeds and solar irradition.  However, these natural fuel sources do not align with large demand centers so the transmission system is needed to efficiently distribute the energy produced.

\begin{table}
\centering
\footnotesize
\begin{tabular}{| p{7cm} | p{7cm} |}
\hline
\bf Previous Conditions 	&	\bf Emerging Conditions 		\\	\hline  	\hline
Fewer, relatively large resources	&	Smaller, more numerous resources		\\	\hline
Long-term, firm contracts 	&	Contracts shorter in duration, more non-firm transactions, fewer long-term firm transactions	\\	\hline
Bulk power transactions relatively stable and predictable	&	Bulk power transactions relatively variable and less predictable	\\	\hline
Assessment of system reliability made from stable base (narrower, more predictable range of potential operating states)	&	Assessment of system reliability made from variable base (wider, less predicatable range of potential operating states)		\\		\hline
Limited and knowledgable set of utility players 	&	More players making more transactions, some with less interconnected operation experience; increasing with retail access	\\	\hline
Unusued transmission capacity and high security margins	&	High transmission utilization and operation closer to security limits	\\	\hline
Limited competition, little incentive for reducing reliability investments	&	Utilities less willing to make investments in transmission reliability that do not increase revenues	\\	\hline
Market rules and reliability rules developed together	&	Market rules undergoing transition, reliability rules developed separately	\\	\hline
Limited wheeling	&	More system throughput	\\	\hline
\end{tabular}
\caption{\small Changing conditions that affect system reliability (from the Northeast outage report \cite{northeast_2003}).}
 \label{tab:change1}
\end{table}

\section{Thus We Care!}
So this is how we will do something about it.

Ultimately, we need to be able to give operators good information and as much time as possible to be able to react to the contingency and avert collapse.  In order to give the operators more time, we must make the system more robust to contingencies.  The primary goal of this research is to be able to optimize various design problems with respect to cascading power failures. 

\subsection{Report Outline}

\subsubsection{Modeling Cascades}
The topological model of the power grid is composed of the various elements in the transmission network as well as their parameters.  While these types of models are simple, they are incapable of capturing effects which are loading dependent. 

The OPA type model is a sequential process in which the power flows for the entire network are calculated in order to determine the loading on the various transmission elements.  This in addition to a deterministic or probablistic failure model is used to outage individual components at each stage in the cascade.  The cascade concludes when no elements are outaged.

Even in its simplest form, this type of model has been shown to capture the criticality effects of blackouts (higher than expected large blackouts) we see in real power systems in aggregate.  As more complexity is added into this model, this type of model can produce reasonable sequences of outages for the system, while remaining accurate in aggregate. 

Building this model requires calculating the power flows.  While the system is not necessarily stable or balanced throughout the cascade process, the balanced, steady state 3 phase power flow equations will be used as a basis for the power flow analysis.  This keeps the calculations tractable and gives valuable information about the various topologies and loading patterns.  In the simplest form of the OPA Model, the power flows will be approximated using the DC (Decoupled) power flow model, which is a linear approximation to the AC power flow equations.

In addition to the OPA model, accessory information can be used to aid in the optimization process.  This includes things such as pseudo-topological statistics (electrical distances and centrality), line outage patterns and recognition, and finally economic information.  Additionally, making a link between reliability and economics requires development of an economic dispatch model.

Design problems will be formed that change aspects of the topology, component parameters, or operation of the system.  Using the OPA model to gauge the response of the system to these changes, we will have formulated a mathematical optimization problem.  These design problems can range from transmission expansions to setting operational reserve levels or even producing protective relay settings for individual components.

\subsubsection{Optimizing Design Problems}
This problem offers several difficulties which make optimization challenging.  

A primary difficulty is the sequential process of the OPA model in which operators make decisions under uncertainty and the decisions effect the future stages and progression of the process.  In addition, the decision dependent outcomes only have a probabilistic effect on the progression of the cascade.  That is, an overloaded line may not fail in all scenarios, so that at each stage a range of outcomes is needed to capture the probabilistic effects.  In order to properly describe the outcomes of the process under a range of scenarios, a multi-stage tree with even a modest number of stages and outcomes explodes quickly.

Another main challenge is that the change of topology of the power system creates nonlinear and nonconvex effects.  It is possible to reduce the cost of electricity by removing transmission elements.  On the other hand, by adding transmission lines, a system capable of meeting demand can become infeasible.  Since the system is constrained by KVL and KCL, each transmission line, while providing a path for electricity flow, in some sense further constrains the system.  It is well known that non-convex behavior is not desirable for optimization problems, and in this case we can have non-convex behavior between each stage of our process, since by definition, each stage before the final has topology changes.

\subsubsection{Multi-Stage Integer Program}
In order to better understand the structure of the problem, it will be formulated as a multi-stage stochastic program with decision-dependent uncertainty.  In order to do this, logical connections between the stages will be made using binary variables, that is variables that can only take the value 0 or 1.  In this way, the decision dependent uncertainty can be modeled explicitly and the probalistic effects can be modeled by sampling the random variables a prior.  As the number of outcomes per stage increases, the better the probablistic effects can be modeled.  Using this type of model, the number of stages has to be decided a prior as well.  This is a major shortcoming as we are concerned with the effects of the worst cascades which can take many stages to complete.  As the number of nodes, $\cN$, is related to the number of outcomes, $a$, and number of stages, $b$, by $\cN \propto a^{b}$, the model quickly becomes intractable. 

 However, if a crude approximation to the cascade process can get similar aggregate effects from the outcome, we will be able to use this formulation as a subproblem.  This can then be embedded in design master problem, an example being transmission expansion, and then solved with an out of the box mixed-integer solver.

\subsubsection{Derivative Free Optimization}
The computational complexity of the first approach grows too fast with respect to increases in model accuracy.  In this approach, the OPA model will be simulated instead of more rigoroursly mathematically defined.  In this we lose some structure of the problem but by decoupling the stages of the OPA model from the master problem, we have a fairly simple monte carlo simulation.  Using variance reduction techniques such as common random numbers we can resolve the outcome of any configuration of the design problems extremely quickly by running a large number of simulations.  In addition, we can parallelize the process in order to evaluate multiple configurations simulataneously.  The outcome of these simulations will be statistics about the magnitude of the blackout for the various contingencies as well as possible sequence of outages.

The optimization field has many algorithms that can use a zeroth order oracle, that is for each configuration the oracle returns a real valued number and may include stochastic variation or numerical error.  In our case the simulation is the oracle and the real valued number could be the expected load shed or the value-at-risk which attempts to capture large tail effects of distributions.  The main types of derivative free optimization are pattern search, model based, and stochastic approximations.  

A pattern search method doesn't attempt to understand anything about the underlying structure but instead tries to search in a specific way such that the function value improves.  Certain patterns can offer local convergences guarantees, that is the solution is at least better than all of those in the neighborhood.  This in combination with a grid search in which the whole space is partitioned and sampled can get you good global solutions, but global convergence will not be provable.  These search methods can be improved by using well positioned exploratory steps and search directions that conform to local topology.

  The model based methods takes a set of sampled points and builds a polynomial model, typically linear or quadratic, and then optimizes this model exactly.  This type of model can have faster local convergence properties, although in some cases it performs much worse than even a simple pattern search.

Stochastic models assume an underlying probability distribution about the function.  In addition, assumptions are made such that by increasing sampling infinitely, the sample approximation converges to the true value.  These models can have characteristics of both pattern search and model based methods, but under the assumption of some stochastic error being present.

\subsubsection{Changing the Market Model}
The third approach will attempt to adjust the current market model in order to put a price on destabilizing effects.  In order to illustrate some of the shortcomings of the current market, two examples will be used.  The first example will look at generators and highlight the fact that they are not getting paid according to their value to the system.  The second will look at demand and highlight that all demands are not equal and thus should not pay the same marginal price.

The first example is of 2 generators at the same bus of a power system and they have equal marginal cost at some point in time.  Generator A is outputting at its maximum output level but generator B is below its maximum output level.  Generator A is only able to ramp down but generator B is able to ramp up or ramp down.  Generator B provides additional flexibility to the system in order to find a least cost power flow and also increase stability of the system.  In today's market, this generator could bid this capacity into regulation services and get paid.  However, if these generators are not used for ancillary services, they get paid at an equal rate despite offering unequal services.   Furthermore, the regulation and reserve markets are set up so that you can bid into the markets as long as you meet some minimum requirements.  This puts unequal services on the same level and when you are finding a least cost solution, you are likely to deploy inferior quality services.  The market should be able to place the correct value on the true varying levels of service.

The second example is of two different demand profiles at the same node and their demand does not affect the locational marginal price (LMP).  Demand A is a fixed demand and doesn't fluctuate at all.  Demand B has an average demand equal to demand A, but half the time is at twice A's level and half the time at 0.  Since they both consume the same amount of power, these demands both pay the same rate.  But these demands place unequal stress on the system.  If all demand was type B, additional resources would need to be put on the grid to meet demand.  Type B demand also increases the regulation needed to ensure reliability of the grid.  Currently, the regulation services bought to support type B demand is paid for by all demand equally.  The market should be able to place the correct price on the varying levels of stress demand puts on the grid.     

The market model needs to account for the cost of volatility to the system.  Currently, this cost is socialized to the whole system through the cost of ancillary services such as reserves and regulation support instead of being bore by those that cause the instabilities.  In order to properly account for these costs and reward those offering these services, a new market model will be made.  The strengths and weakness of this model will be showed, in particular the effects it has on the likelihood of  cascading power failures.

A key tool in building the market model will be critical points. The reliability of the electricity grid can be controlled by varying the distance between the operating positions and various critical points.  Using a list of contingencies, costs associated with each contingency can be measured by its new distance to bad operating points.  Once we have a measure, we will have a value to place on entities causing stress to the system and reward those providing the services.  This market will be able to take over both the regulation and reserve markets.  Regulation and reserve markets are incapable of properly pricing reliability issues, which is exactly what they are designed to do.  Instead, due to having set points for how much reserve and regulation they have, there is no ability to toggle how reliable the system is.  However, we know that the reliability of power systems fluctuates seasonally and daily, but the level of support the system is getting is not.  In addition, with the ability to have a tangible measure for reliability, the consumers of electricity will have a better quality product.  The power systems will now have the ability to adjust the cost of electricity for a trade-off in reliability.  This will link the economic and reliability issues of the power grid.  It will allow a uniform system to apply to various tough problems in electricity markets such as demand response and energy storage.

If we look at an uncertain demand and note it has some average consumption rate over a time interval and a certain amount of variance from its mean, we can see that a load with zero variance will be the cheapest possible load.  Any positive amount of variance will move the system closer to critical points at some point in time over the given interval.  Loads with extremely high variance will have to pay even more as they will move the system even closer to the bad operating points.  Energy storage devices and demand response will be able to play an important role in these markets as well.  It is unlikely that we stumbled onto the reliability frontier and this model should help the system move closer.  

